{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9366344,"sourceType":"datasetVersion","datasetId":5679841}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-26T03:18:33.777922Z","iopub.execute_input":"2024-09-26T03:18:33.778439Z","iopub.status.idle":"2024-09-26T03:18:33.790906Z","shell.execute_reply.started":"2024-09-26T03:18:33.778393Z","shell.execute_reply":"2024-09-26T03:18:33.789558Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/top-1500-games-on-steam-by-revenue-09-09-2024/Steam_2024_bestRevenue_1500.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:33.835835Z","iopub.execute_input":"2024-09-26T03:18:33.836303Z","iopub.status.idle":"2024-09-26T03:18:33.865986Z","shell.execute_reply.started":"2024-09-26T03:18:33.836259Z","shell.execute_reply":"2024-09-26T03:18:33.864426Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's find the number of null values\ndf.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:33.894529Z","iopub.execute_input":"2024-09-26T03:18:33.895567Z","iopub.status.idle":"2024-09-26T03:18:33.905983Z","shell.execute_reply.started":"2024-09-26T03:18:33.895517Z","shell.execute_reply":"2024-09-26T03:18:33.904272Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Since the number of rows with empty values is very small, we can simply exclude them from our analysis","metadata":{}},{"cell_type":"code","source":"df = df.dropna()\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:33.945852Z","iopub.execute_input":"2024-09-26T03:18:33.946301Z","iopub.status.idle":"2024-09-26T03:18:33.966720Z","shell.execute_reply.started":"2024-09-26T03:18:33.946259Z","shell.execute_reply":"2024-09-26T03:18:33.965135Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:33.991684Z","iopub.execute_input":"2024-09-26T03:18:33.992118Z","iopub.status.idle":"2024-09-26T03:18:33.998298Z","shell.execute_reply.started":"2024-09-26T03:18:33.992076Z","shell.execute_reply":"2024-09-26T03:18:33.996580Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sns.displot(data=df, x=df['price'], stat='count', bins='auto')\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Histogram\nsns.histplot(data=df, x='price', stat='count', bins='auto', ax=axes[0])\naxes[0].set_title('Histogram of Prices')\n\n# Box Plot\nsns.boxplot(data=df, y='price', ax=axes[1])\naxes[1].set_title('Box Plot of Prices')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:34.028269Z","iopub.execute_input":"2024-09-26T03:18:34.028717Z","iopub.status.idle":"2024-09-26T03:18:34.762325Z","shell.execute_reply.started":"2024-09-26T03:18:34.028674Z","shell.execute_reply":"2024-09-26T03:18:34.759994Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We see that the average price of a game is close to 20 dollars**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(24,12))\nsns.pairplot(data=df,\n                  x_vars=['avgPlaytime', 'copiesSold','reviewScore', 'revenue'],\n                  y_vars=['price'],\n                  kind=\"scatter\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:34.764856Z","iopub.execute_input":"2024-09-26T03:18:34.765416Z","iopub.status.idle":"2024-09-26T03:18:35.820408Z","shell.execute_reply.started":"2024-09-26T03:18:34.765353Z","shell.execute_reply":"2024-09-26T03:18:35.819249Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**We observe that there is no correlation between any of avgPlaytime, reviewScore, copiesSold, revenue, and price**","metadata":{}},{"cell_type":"code","source":"df['publishers'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:35.822244Z","iopub.execute_input":"2024-09-26T03:18:35.822781Z","iopub.status.idle":"2024-09-26T03:18:35.834980Z","shell.execute_reply.started":"2024-09-26T03:18:35.822725Z","shell.execute_reply":"2024-09-26T03:18:35.833762Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['publisherClass'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:35.840233Z","iopub.execute_input":"2024-09-26T03:18:35.840667Z","iopub.status.idle":"2024-09-26T03:18:35.851336Z","shell.execute_reply.started":"2024-09-26T03:18:35.840624Z","shell.execute_reply":"2024-09-26T03:18:35.849948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['developers'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:35.852937Z","iopub.execute_input":"2024-09-26T03:18:35.853354Z","iopub.status.idle":"2024-09-26T03:18:35.870836Z","shell.execute_reply.started":"2024-09-26T03:18:35.853298Z","shell.execute_reply":"2024-09-26T03:18:35.869655Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# We can remove the steamID and the name as they won't be useful in our analysis\n\ndf.drop(columns = ['name','steamId'], inplace=True)\ndf\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:35.872781Z","iopub.execute_input":"2024-09-26T03:18:35.873204Z","iopub.status.idle":"2024-09-26T03:18:35.899829Z","shell.execute_reply.started":"2024-09-26T03:18:35.873128Z","shell.execute_reply":"2024-09-26T03:18:35.898458Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Before removing the releaseDate column, we may need to find the number of days that have passed since the game was released because the older the game is the lower its' price becomes\ndf['releaseDate'] = pd.to_datetime(df['releaseDate'], format='%d-%m-%Y')\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:35.901405Z","iopub.execute_input":"2024-09-26T03:18:35.901884Z","iopub.status.idle":"2024-09-26T03:18:35.926920Z","shell.execute_reply.started":"2024-09-26T03:18:35.901841Z","shell.execute_reply":"2024-09-26T03:18:35.925323Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now, we can add a column that shows how many days have passed since the game's release date\nfrom datetime import datetime\n\ndf['days_since_release'] = (datetime.now() - df['releaseDate']).dt.days\ndf.drop(columns= ['releaseDate'], inplace=True)\ndf = df.reset_index(drop=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:35.928581Z","iopub.execute_input":"2024-09-26T03:18:35.929037Z","iopub.status.idle":"2024-09-26T03:18:35.957219Z","shell.execute_reply.started":"2024-09-26T03:18:35.928994Z","shell.execute_reply":"2024-09-26T03:18:35.955987Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n#Extract categorical columns from the dataframe\n#Here we extract the columns with object datatype as they are the categorical columns\ncategorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n\n#Initialize OneHotEncoder\nencoder = OneHotEncoder(sparse_output=False)\n\n# Apply one-hot encoding to the categorical columns\none_hot_encoded = encoder.fit_transform(df[categorical_columns])\n\n#Create a DataFrame with the one-hot encoded columns\n#We use get_feature_names_out() to get the column names for the encoded data\none_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n\n# Concatenate the one-hot encoded dataframe with the original dataframe\ndf_encoded = pd.concat([df, one_hot_df], axis=1)\n\n# Drop the original categorical columns\ndf_encoded = df_encoded.drop(categorical_columns, axis=1)\ndf_encoded","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:35.958624Z","iopub.execute_input":"2024-09-26T03:18:35.958989Z","iopub.status.idle":"2024-09-26T03:18:36.112949Z","shell.execute_reply.started":"2024-09-26T03:18:35.958950Z","shell.execute_reply":"2024-09-26T03:18:36.111560Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr = df[['copiesSold', 'revenue','avgPlaytime','reviewScore','days_since_release', 'price']].corr()\n#sns.heatmap(corr, annot=True, fmt=\".4f\", cmap=sns.color_palette(\"YlOrBr\", as_cmap=True))\nsns.heatmap(corr, annot=True, fmt=\".4f\", cmap=sns.color_palette(\"rocket_r\", as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:36.119286Z","iopub.execute_input":"2024-09-26T03:18:36.119951Z","iopub.status.idle":"2024-09-26T03:18:36.644548Z","shell.execute_reply.started":"2024-09-26T03:18:36.119886Z","shell.execute_reply":"2024-09-26T03:18:36.643223Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We observe that the highest correlation between the variables is the correlation between **revenue** and **copiesSold** which **0.6277** which is not very high whereas the rest of the correlations vary between **0.01** and **0.1**. This indicaes that as the number of copies sold increases, the revenue tends to increase as well which is logical of course. \n\nAs for the rest of the variables whose correlations vary between **0.01** and **0.1**, these ones are very small, which suggests that there is little to no linear relationship between those variables. This could indicate that the rest of the variables in this dataset are rather independent from each other or simply that whatever relationship exist between them is non-linear or determined by other factors not captured by the data set.\n\nOverall, aparat from **revenue** and **copiesSold**,this indicates that the variables are linearly independant and we can go ahead and use them in the analysis","metadata":{}},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"markdown","source":"## Basic Regression Models","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nfeatures = df_encoded.loc[:, df_encoded.columns != 'price']\ntarget = df_encoded[['price']]\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:36.645984Z","iopub.execute_input":"2024-09-26T03:18:36.646358Z","iopub.status.idle":"2024-09-26T03:18:36.680181Z","shell.execute_reply.started":"2024-09-26T03:18:36.646319Z","shell.execute_reply":"2024-09-26T03:18:36.678871Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reg = LinearRegression()\nreg.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:36.682116Z","iopub.execute_input":"2024-09-26T03:18:36.682698Z","iopub.status.idle":"2024-09-26T03:18:39.020107Z","shell.execute_reply.started":"2024-09-26T03:18:36.682643Z","shell.execute_reply":"2024-09-26T03:18:39.016882Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n\n# Make Predictions on the test data\ny_pred = reg.predict(X_test)\n\n# Calculate the mean squared error and R^2 score\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R^2 Score: {r2}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:39.024962Z","iopub.execute_input":"2024-09-26T03:18:39.030302Z","iopub.status.idle":"2024-09-26T03:18:39.125032Z","shell.execute_reply.started":"2024-09-26T03:18:39.030202Z","shell.execute_reply":"2024-09-26T03:18:39.123769Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is a relativley high MSE, let's now try the regression with the other regression models that we could use ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\n\n\ndef evaluate_data(X_train, y_train, X_test, y_test):\n    models = {\n        \"Linear Regression\": LinearRegression(),\n        \"Ridge Regression\": Ridge(),\n        \"Lasso Regression\": Lasso(),\n        \"ElasticNet Regression\": ElasticNet(),\n        \"Decision Tree Regressor\": DecisionTreeRegressor(),\n        \"Random Forest Regressor\": RandomForestRegressor(),\n        \"Gradient Boosting Regressor\": GradientBoostingRegressor(),\n        \"Support Vector Regressor\": SVR()\n    }\n\n    # Train and test each model\n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        mse = mean_squared_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n        print(f\"{name} - Mean Squared Error: {mse:.4f} || R^2 Score: {r2:.4f}\")\n\nevaluate_data(X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:39.127069Z","iopub.execute_input":"2024-09-26T03:18:39.128670Z","iopub.status.idle":"2024-09-26T03:18:55.852363Z","shell.execute_reply.started":"2024-09-26T03:18:39.128608Z","shell.execute_reply":"2024-09-26T03:18:55.850824Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It appears that the **Gradient Boosting Regressor** algorithm is the best performing model. And from the $R^2$ socre, this model can explain almost **70%** of the data. But let's see if we can improve the model further by applying standardization on the dataset","metadata":{}},{"cell_type":"markdown","source":"## Standardize the Dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nnumeric_features = ['copiesSold', 'revenue','avgPlaytime','reviewScore','days_since_release']\nscaler = StandardScaler()\n\n# Transform the data using data normalization on the features\n\nnormalized_df = pd.DataFrame(scaler.fit_transform(df_encoded[numeric_features]))\nnormalized_df.columns = numeric_features\n\nnormalized_df = pd.concat([normalized_df,df_encoded['price'], one_hot_df], axis=1)\nnormalized_df","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:55.854513Z","iopub.execute_input":"2024-09-26T03:18:55.855017Z","iopub.status.idle":"2024-09-26T03:18:55.932229Z","shell.execute_reply.started":"2024-09-26T03:18:55.854961Z","shell.execute_reply":"2024-09-26T03:18:55.931078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now, let's split the data again into a training and a test set\n\n# Caputre the features and the target variables seperatley\nfeatures = normalized_df.loc[:, normalized_df.columns != 'price']\ntarget = normalized_df[['price']]\n\n# Split the data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\n# Evaluate the model on the normalized data\nevaluate_data(X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:18:55.933799Z","iopub.execute_input":"2024-09-26T03:18:55.934181Z","iopub.status.idle":"2024-09-26T03:19:11.442937Z","shell.execute_reply.started":"2024-09-26T03:18:55.934140Z","shell.execute_reply":"2024-09-26T03:19:11.441575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MinMaxStandardization","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nnumeric_features = ['copiesSold', 'revenue','avgPlaytime','reviewScore','days_since_release']\nscaler = MinMaxScaler()\n\n# Transform the data using data normalization on the features\n\nmin_max_df = pd.DataFrame(scaler.fit_transform(df_encoded[numeric_features]))\nmin_max_df.columns = numeric_features\n\nmin_max_df = pd.concat([min_max_df,df_encoded['price'], one_hot_df], axis=1)\nmin_max_df","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:19:11.444516Z","iopub.execute_input":"2024-09-26T03:19:11.444909Z","iopub.status.idle":"2024-09-26T03:19:11.519583Z","shell.execute_reply.started":"2024-09-26T03:19:11.444868Z","shell.execute_reply":"2024-09-26T03:19:11.518057Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now, let's split the data again into a training and a test set\n\n# Caputre the features and the target variables seperatley\nfeatures = min_max_df.loc[:, min_max_df.columns != 'price']\ntarget = min_max_df[['price']]\n\n# Split the data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\n# Evaluate the model on the normalized data\nevaluate_data(X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T03:19:11.521017Z","iopub.execute_input":"2024-09-26T03:19:11.521391Z","iopub.status.idle":"2024-09-26T03:19:27.149744Z","shell.execute_reply.started":"2024-09-26T03:19:11.521349Z","shell.execute_reply":"2024-09-26T03:19:27.148377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**From what we see above, it appears that standardization doesn't make a difference in the model performance**","metadata":{}}]}